{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 98\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[39mreturn\u001b[39;00m scores\n\u001b[1;32m     97\u001b[0m t4_2_agent \u001b[39m=\u001b[39m T4_2_FlappyAgent()\n\u001b[0;32m---> 98\u001b[0m t4_2_train(nb_episodes\u001b[39m=\u001b[39;49m\u001b[39m100000\u001b[39;49m, agent\u001b[39m=\u001b[39;49mt4_2_agent)\n\u001b[1;32m     99\u001b[0m t4_2_scores \u001b[39m=\u001b[39m t4_2_evaluate_agent(t4_2_agent, nb_episodes\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m)\n\u001b[1;32m    100\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mScores from 10 simulated games:\u001b[39m\u001b[39m\"\u001b[39m, t4_2_scores)\n",
      "Cell \u001b[0;32mIn[2], line 53\u001b[0m, in \u001b[0;36mt4_2_train\u001b[0;34m(nb_episodes, agent)\u001b[0m\n\u001b[1;32m     51\u001b[0m total_reward \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m     52\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m env\u001b[39m.\u001b[39mgame_over():\n\u001b[0;32m---> 53\u001b[0m     action \u001b[39m=\u001b[39m agent\u001b[39m.\u001b[39;49mtraining_policy(state)\n\u001b[1;32m     54\u001b[0m     reward \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mact(env\u001b[39m.\u001b[39mgetActionSet()[action])\n\u001b[1;32m     55\u001b[0m     new_state \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mgame\u001b[39m.\u001b[39mgetGameState()\n",
      "Cell \u001b[0;32mIn[2], line 38\u001b[0m, in \u001b[0;36mT4_2_FlappyAgent.training_policy\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[39mif\u001b[39;00m random\u001b[39m.\u001b[39mrandom() \u001b[39m<\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mepsilon:\n\u001b[1;32m     37\u001b[0m     \u001b[39mreturn\u001b[39;00m random\u001b[39m.\u001b[39mrandint(\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m)\n\u001b[0;32m---> 38\u001b[0m \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39;49margmax(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mq_table[state_discrete])\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/numpy/core/fromnumeric.py:1229\u001b[0m, in \u001b[0;36margmax\u001b[0;34m(a, axis, out, keepdims)\u001b[0m\n\u001b[1;32m   1142\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1143\u001b[0m \u001b[39mReturns the indices of the maximum values along an axis.\u001b[39;00m\n\u001b[1;32m   1144\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1226\u001b[0m \u001b[39m(2, 1, 4)\u001b[39;00m\n\u001b[1;32m   1227\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1228\u001b[0m kwds \u001b[39m=\u001b[39m {\u001b[39m'\u001b[39m\u001b[39mkeepdims\u001b[39m\u001b[39m'\u001b[39m: keepdims} \u001b[39mif\u001b[39;00m keepdims \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m np\u001b[39m.\u001b[39m_NoValue \u001b[39melse\u001b[39;00m {}\n\u001b[0;32m-> 1229\u001b[0m \u001b[39mreturn\u001b[39;00m _wrapfunc(a, \u001b[39m'\u001b[39;49m\u001b[39margmax\u001b[39;49m\u001b[39m'\u001b[39;49m, axis\u001b[39m=\u001b[39;49maxis, out\u001b[39m=\u001b[39;49mout, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/numpy/core/fromnumeric.py:56\u001b[0m, in \u001b[0;36m_wrapfunc\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     54\u001b[0m bound \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(obj, method, \u001b[39mNone\u001b[39;00m)\n\u001b[1;32m     55\u001b[0m \u001b[39mif\u001b[39;00m bound \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m---> 56\u001b[0m     \u001b[39mreturn\u001b[39;00m _wrapit(obj, method, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m     58\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     59\u001b[0m     \u001b[39mreturn\u001b[39;00m bound(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/numpy/core/fromnumeric.py:45\u001b[0m, in \u001b[0;36m_wrapit\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m:\n\u001b[1;32m     44\u001b[0m     wrap \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m---> 45\u001b[0m result \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(asarray(obj), method)(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[1;32m     46\u001b[0m \u001b[39mif\u001b[39;00m wrap:\n\u001b[1;32m     47\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(result, mu\u001b[39m.\u001b[39mndarray):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Task 4.2\n",
    "\n",
    "from ple.games.flappybird import FlappyBird\n",
    "from ple import PLE\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import defaultdict\n",
    "\n",
    "class T4_2_FlappyAgent:\n",
    "    def __init__(self):\n",
    "        self.epsilon = 0.1\n",
    "        self.alpha = 0.1\n",
    "        self.gamma = 1.0\n",
    "        self.q_table = defaultdict(lambda: [0.0, 0.0])\n",
    "\n",
    "    def reward_values(self):\n",
    "        return {\"positive\": 1.0, \"tick\": 0.0, \"loss\": -5.0}\n",
    "\n",
    "    def discretize_state(self, state):\n",
    "        player_y = int(state['player_y'] / 15)\n",
    "        next_pipe_top_y = int(state['next_pipe_top_y'] / 15)\n",
    "        next_pipe_dist_to_player = int(state['next_pipe_dist_to_player'] / 15)\n",
    "        player_vel = state['player_vel']\n",
    "        return (player_y, next_pipe_top_y, next_pipe_dist_to_player, player_vel)\n",
    "\n",
    "    def observe(self, s1, a, r, s2, end):\n",
    "        s1_discrete = self.discretize_state(s1)\n",
    "        s2_discrete = self.discretize_state(s2)\n",
    "        max_q_s2 = max(self.q_table[s2_discrete]) if not end else 0\n",
    "        self.q_table[s1_discrete][a] += self.alpha * (r + self.gamma * max_q_s2 - self.q_table[s1_discrete][a])\n",
    "\n",
    "    def training_policy(self, state):\n",
    "        state_discrete = self.discretize_state(state)\n",
    "        if random.random() < self.epsilon:\n",
    "            return random.randint(0, 1)\n",
    "        return np.argmax(self.q_table[state_discrete])\n",
    "\n",
    "    def policy(self, state):\n",
    "        state_discrete = self.discretize_state(state)\n",
    "        return np.argmax(self.q_table[state_discrete])\n",
    "\n",
    "def t4_2_train(nb_episodes, agent):\n",
    "    reward_values = agent.reward_values()\n",
    "    env = PLE(FlappyBird(), fps=30, display_screen=False, force_fps=True, rng=None, reward_values=reward_values)\n",
    "    env.init()\n",
    "    rewards_per_episode = []\n",
    "    for episode in range(nb_episodes):\n",
    "        state = env.game.getGameState()\n",
    "        total_reward = 0\n",
    "        while not env.game_over():\n",
    "            action = agent.training_policy(state)\n",
    "            reward = env.act(env.getActionSet()[action])\n",
    "            new_state = env.game.getGameState()\n",
    "            agent.observe(state, action, reward, new_state, env.game_over())\n",
    "            state = new_state\n",
    "            total_reward += reward\n",
    "        rewards_per_episode.append(total_reward)\n",
    "        env.reset_game()\n",
    "    t4_2_plot_training_progress(rewards_per_episode)\n",
    "\n",
    "def t4_2_plot_training_progress(rewards):\n",
    "    sns.set(style=\"whitegrid\")\n",
    "    smoothed_rewards = np.convolve(rewards, np.ones(10) / 10, mode='valid')\n",
    "    x_vals = np.arange(len(smoothed_rewards))\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.lineplot(x=x_vals, y=smoothed_rewards, lw=2, label=\"Smoothed Reward\")\n",
    "    log_x = np.log(x_vals + 1)\n",
    "    coeffs = np.polyfit(log_x, smoothed_rewards, 1)\n",
    "    fitted_y = coeffs[0] * log_x + coeffs[1]\n",
    "    plt.plot(x_vals, fitted_y, color=\"red\", label=\"Logarithmic Best Fit Line\")\n",
    "    plt.xlabel(\"Episodes\")\n",
    "    plt.ylabel(\"Total Reward per Episode\")\n",
    "    plt.title(\"Training Progress of Q-Learning Agent in Flappy Bird\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "def t4_2_evaluate_agent(agent, nb_episodes=10):\n",
    "    env = PLE(FlappyBird(), fps=30, display_screen=False, force_fps=True, rng=None)\n",
    "    env.init()\n",
    "    scores = []\n",
    "    for episode in range(nb_episodes):\n",
    "        state = env.game.getGameState()\n",
    "        total_reward = 0\n",
    "        while not env.game_over():\n",
    "            action = agent.policy(state)\n",
    "            reward = env.act(env.getActionSet()[action])\n",
    "            state = env.game.getGameState()\n",
    "            total_reward += reward\n",
    "        scores.append(total_reward)\n",
    "        print(f\"Score for episode {episode + 1}: {total_reward}\")\n",
    "        env.reset_game()\n",
    "    return scores\n",
    "\n",
    "t4_2_agent = T4_2_FlappyAgent()\n",
    "t4_2_train(nb_episodes=100000, agent=t4_2_agent)\n",
    "t4_2_scores = t4_2_evaluate_agent(t4_2_agent, nb_episodes=10)\n",
    "print(\"Scores from 10 simulated games:\", t4_2_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 4.3\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from ple.games.flappybird import FlappyBird\n",
    "from ple import PLE\n",
    "\n",
    "class T4_3_FlappyAgent:\n",
    "    def __init__(self, state_size=4, action_size=2):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=1000)\n",
    "        self.gamma = 0.99\n",
    "        self.epsilon = 1.0\n",
    "        self.epsilon_min = 0.1\n",
    "        self.epsilon_decay = 0.99995\n",
    "        self.learning_rate = 0.01\n",
    "        self.batch_size = 100\n",
    "        self.update_target_frequency = 100\n",
    "        self.model = MLPRegressor(hidden_layer_sizes=(100, 10), activation='logistic', learning_rate_init=self.learning_rate)\n",
    "        self.target_model = MLPRegressor(hidden_layer_sizes=(100, 10), activation='logistic', learning_rate_init=self.learning_rate)\n",
    "        self.initial_fit()\n",
    "\n",
    "    def initial_fit(self):\n",
    "        dummy_X = np.random.rand(10, self.state_size)\n",
    "        dummy_y = np.random.rand(10, self.action_size)\n",
    "        self.model.fit(dummy_X, dummy_y)\n",
    "        self.target_model.fit(dummy_X, dummy_y)\n",
    "\n",
    "    def reward_values(self):\n",
    "        return {\"positive\": 1.0, \"tick\": 0.0, \"loss\": -5.0}\n",
    "\n",
    "    def normalize_state(self, state):\n",
    "        max_values = {'player_y': 512, 'next_pipe_top_y': 512, 'next_pipe_dist_to_player': 288, 'player_vel': 10}\n",
    "        return [\n",
    "            (state['player_y'] / max_values['player_y']) * 2 - 1,\n",
    "            (state['next_pipe_top_y'] / max_values['next_pipe_top_y']) * 2 - 1,\n",
    "            (state['next_pipe_dist_to_player'] / max_values['next_pipe_dist_to_player']) * 2 - 1,\n",
    "            (state['player_vel'] / max_values['player_vel']) * 2 - 1,\n",
    "        ]\n",
    "\n",
    "    def observe(self, s1, a, r, s2, end):\n",
    "        s1 = self.normalize_state(s1)\n",
    "        s2 = self.normalize_state(s2)\n",
    "        self.memory.append((s1, a, r, s2, end))\n",
    "        if len(self.memory) >= self.batch_size:\n",
    "            self.replay()\n",
    "\n",
    "    def replay(self):\n",
    "        batch = random.sample(self.memory, self.batch_size)\n",
    "        states = np.array([item[0] for item in batch])\n",
    "        actions = [item[1] for item in batch]\n",
    "        rewards = np.array([item[2] for item in batch])\n",
    "        next_states = np.array([item[3] for item in batch])\n",
    "        ends = np.array([item[4] for item in batch])\n",
    "        q_values_next = self.target_model.predict(next_states).max(axis=1)\n",
    "        targets = rewards + (self.gamma * q_values_next * (1 - ends))\n",
    "        q_values = self.model.predict(states)\n",
    "\n",
    "        for i, action in enumerate(actions):\n",
    "            q_values[i, action] = targets[i] if not ends[i] else rewards[i]\n",
    "\n",
    "        self.model.partial_fit(states, q_values)\n",
    "\n",
    "        if random.randint(1, self.update_target_frequency) == 1:\n",
    "            self.update_target_network()\n",
    "\n",
    "    def update_target_network(self):\n",
    "        self.target_model.coefs_ = self.model.coefs_\n",
    "        self.target_model.intercepts_ = self.model.intercepts_\n",
    "\n",
    "    def training_policy(self, state):\n",
    "        state = self.normalize_state(state)\n",
    "        if random.uniform(0, 1) < self.epsilon:\n",
    "            return random.randint(0, self.action_size - 1)\n",
    "        q_values = self.model.predict([state])\n",
    "        return int(np.argmax(q_values))\n",
    "\n",
    "    def policy(self, state):\n",
    "        state = self.normalize_state(state)\n",
    "        q_values = self.model.predict([state])\n",
    "        return int(np.argmax(q_values))\n",
    "\n",
    "    def train_on_episode_end(self):\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "def t4_3_train(nb_episodes, agent):\n",
    "    reward_values = agent.reward_values()\n",
    "    env = PLE(FlappyBird(), fps=30, display_screen=False, force_fps=True, reward_values=reward_values, rng=None)\n",
    "    env.init()\n",
    "    score = 0\n",
    "    scores = []\n",
    "    max_score = 0\n",
    "    print(f\"Starting training for {nb_episodes} episodes\")\n",
    "\n",
    "    for episode in range(nb_episodes):\n",
    "        state = env.game.getGameState()\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            action = agent.training_policy(state)\n",
    "            reward = env.act(env.getActionSet()[action])\n",
    "            new_state = env.game.getGameState()\n",
    "            done = env.game_over()\n",
    "            agent.observe(state, action, reward, new_state, done)\n",
    "            state = new_state\n",
    "            score += reward\n",
    "\n",
    "        scores.append(score)\n",
    "        if score > max_score:\n",
    "            max_score = score\n",
    "        agent.train_on_episode_end()\n",
    "        env.reset_game()\n",
    "        score = 0\n",
    "\n",
    "        if (episode + 1) % 1000 == 0:\n",
    "            avg_score_1000 = sum(scores[-1000:]) / 1000\n",
    "            print(f\"Episode {episode + 1}: Highest Score: {max_score:.2f}, Average Score over last 1000 episodes: {avg_score_1000:.2f}\")\n",
    "\n",
    "    print(\"Training completed.\")\n",
    "\n",
    "def t4_3_evaluate(agent, nb_episodes=5):\n",
    "    reward_values = agent.reward_values()\n",
    "    env = PLE(FlappyBird(), fps=30, display_screen=False, force_fps=True, reward_values=reward_values, rng=None)\n",
    "    env.init()\n",
    "    print(\"Evaluation results for 5 runs:\")\n",
    "\n",
    "    for episode in range(nb_episodes):\n",
    "        score = 0\n",
    "        state = env.game.getGameState()\n",
    "        done = False\n",
    "        while not done:\n",
    "            action = agent.policy(state)\n",
    "            reward = env.act(env.getActionSet()[action])\n",
    "            state = env.game.getGameState()\n",
    "            score += reward\n",
    "            done = env.game_over()\n",
    "        \n",
    "        print(f\"Score for evaluation episode {episode + 1}: {score:.2f}\")\n",
    "        env.reset_game()\n",
    "\n",
    "t4_3_agent = T4_3_FlappyAgent()\n",
    "t4_3_train(10000, t4_3_agent)\n",
    "t4_3_evaluate(t4_3_agent)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 4.4\n",
    "\n",
    "from ple.games.flappybird import FlappyBird\n",
    "from ple import PLE\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import defaultdict\n",
    "\n",
    "class T4_4_FlappyAgent:\n",
    "    def __init__(self, epsilon=0.1, alpha=0.1, gamma=1.0, reward_structure=None, resolution=15):\n",
    "        self.epsilon = epsilon\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.q_table = defaultdict(lambda: [0.0, 0.0])\n",
    "        self.reward_structure = reward_structure if reward_structure else {\"positive\": 1.0, \"tick\": 0.0, \"loss\": -5.0}\n",
    "        self.resolution = resolution\n",
    "\n",
    "    def reward_values(self):\n",
    "        return self.reward_structure\n",
    "\n",
    "    def discretize_state(self, state):\n",
    "        player_y = int(state['player_y'] / self.resolution)\n",
    "        next_pipe_top_y = int(state['next_pipe_top_y'] / self.resolution)\n",
    "        next_pipe_dist_to_player = int(state['next_pipe_dist_to_player'] / self.resolution)\n",
    "        player_vel = state['player_vel']\n",
    "        return (player_y, next_pipe_top_y, next_pipe_dist_to_player, player_vel)\n",
    "\n",
    "    def observe(self, s1, a, r, s2, end):\n",
    "        s1_discrete = self.discretize_state(s1)\n",
    "        s2_discrete = self.discretize_state(s2)\n",
    "        max_q_s2 = max(self.q_table[s2_discrete]) if not end else 0\n",
    "        self.q_table[s1_discrete][a] += self.alpha * (r + self.gamma * max_q_s2 - self.q_table[s1_discrete][a])\n",
    "\n",
    "    def training_policy(self, state):\n",
    "        state_discrete = self.discretize_state(state)\n",
    "        if random.random() < self.epsilon:\n",
    "            return random.randint(0, 1)\n",
    "        return np.argmax(self.q_table[state_discrete])\n",
    "\n",
    "    def policy(self, state):\n",
    "        state_discrete = self.discretize_state(state)\n",
    "        return np.argmax(self.q_table[state_discrete])\n",
    "\n",
    "def t4_4_train(nb_episodes, agent):\n",
    "    reward_values = agent.reward_values()\n",
    "    env = PLE(FlappyBird(), fps=30, display_screen=False, force_fps=True, rng=None, reward_values=reward_values)\n",
    "    env.init()\n",
    "    episode_scores = []\n",
    "    for episode in range(nb_episodes):\n",
    "        state = env.game.getGameState()\n",
    "        score = 0\n",
    "        while not env.game_over():\n",
    "            action = agent.training_policy(state)\n",
    "            reward = env.act(env.getActionSet()[action])\n",
    "            new_state = env.game.getGameState()\n",
    "            agent.observe(state, action, reward, new_state, env.game_over())\n",
    "            state = new_state\n",
    "            score += 1 if reward == reward_values[\"positive\"] else 0  \n",
    "        episode_scores.append(score)\n",
    "        env.reset_game()\n",
    "    return episode_scores\n",
    "\n",
    "def t4_4_simulate_games(agent, nb_games=2):\n",
    "    env = PLE(FlappyBird(), fps=30, display_screen=False, force_fps=True, rng=None)\n",
    "    env.init()\n",
    "    scores = []\n",
    "    for _ in range(nb_games):\n",
    "        state = env.game.getGameState()\n",
    "        score = 0\n",
    "        while not env.game_over():\n",
    "            action = agent.policy(state)\n",
    "            reward = env.act(env.getActionSet()[action])\n",
    "            state = env.game.getGameState()\n",
    "            score += 1 if reward == agent.reward_structure[\"positive\"] else 0\n",
    "        scores.append(score)\n",
    "        env.reset_game()\n",
    "    return scores\n",
    "\n",
    "def t4_4_plot_experiment_results(experiment_results):\n",
    "    sns.set(style=\"whitegrid\")\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    for label, scores in experiment_results.items():\n",
    "        smoothed_scores = np.convolve(scores, np.ones(10) / 10, mode='valid')\n",
    "        x = np.arange(len(smoothed_scores))\n",
    "        polynomial_coefficients = np.polyfit(x, smoothed_scores, 3)\n",
    "        polynomial = np.poly1d(polynomial_coefficients)\n",
    "        plt.plot(smoothed_scores, label=label)\n",
    "        plt.plot(x, polynomial(x), linestyle=\"--\")\n",
    "    plt.xlabel(\"Episodes\")\n",
    "    plt.ylabel(\"Average Score per Episode\")\n",
    "    plt.title(\"Comparison of Agent Performance with Different Parameters\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "def t4_4_run_experiments():\n",
    "    nb_episodes = 30000\n",
    "    experiments = [\n",
    "        {\"epsilon\": 0.1, \"alpha\": 0.1, \"gamma\": 1.0, \"label\": \"Baseline\"},\n",
    "        {\"epsilon\": 0.01, \"alpha\": 0.1, \"gamma\": 1.0, \"label\": \"Very Low Epsilon (0.01)\"},\n",
    "        {\"epsilon\": 0.3, \"alpha\": 0.1, \"gamma\": 1.0, \"label\": \"High Epsilon (0.3)\"},\n",
    "        {\"epsilon\": 0.1, \"alpha\": 0.05, \"gamma\": 1.0, \"label\": \"Lower Alpha (0.05)\"},\n",
    "        {\"epsilon\": 0.1, \"alpha\": 0.2, \"gamma\": 1.0, \"label\": \"Higher Alpha (0.2)\"},\n",
    "        {\"epsilon\": 0.1, \"alpha\": 0.1, \"gamma\": 0.95, \"label\": \"Lower Gamma (0.90)\"},\n",
    "        {\"epsilon\": 0.1, \"alpha\": 0.1, \"gamma\": 1.0, \"reward_structure\": {\"positive\": 2.0, \"tick\": 0.1, \"loss\": -5.0}, \"label\": \"Modified Rewards (Positive=2.0)\"},\n",
    "        {\"epsilon\": 0.1, \"alpha\": 0.1, \"gamma\": 1.0, \"reward_structure\": {\"positive\": 1.0, \"tick\": 0.1, \"loss\": -10.0}, \"label\": \"Harsher Loss (-10)\"},\n",
    "        {\"epsilon\": 0.1, \"alpha\": 0.1, \"gamma\": 1.0, \"resolution\": 10, \"label\": \"Higher State Resolution (10)\"},\n",
    "        {\"epsilon\": 0.1, \"alpha\": 0.1, \"gamma\": 1.0, \"resolution\": 20, \"label\": \"Lower State Resolution (20)\"}\n",
    "    ]\n",
    "    experiment_results = {}\n",
    "    simulation_scores = {}\n",
    "\n",
    "    for exp in experiments:\n",
    "        print(f\"Running experiment: {exp['label']}\")\n",
    "        agent = T4_4_FlappyAgent(\n",
    "            epsilon=exp.get(\"epsilon\", 0.1),\n",
    "            alpha=exp.get(\"alpha\", 0.1),\n",
    "            gamma=exp.get(\"gamma\", 1.0),\n",
    "            reward_structure=exp.get(\"reward_structure\", None),\n",
    "            resolution=exp.get(\"resolution\", 15)\n",
    "        )\n",
    "        episode_scores = t4_4_train(nb_episodes, agent)\n",
    "        experiment_results[exp[\"label\"]] = episode_scores\n",
    "        simulation_scores[exp[\"label\"]] = t4_4_simulate_games(agent, nb_games=5)\n",
    "    \n",
    "    t4_4_plot_experiment_results(experiment_results)\n",
    "    \n",
    "    print(\"Simulation scores for each configuration:\")\n",
    "    for label, scores in simulation_scores.items():\n",
    "        print(f\"{label}: {scores}\")\n",
    "\n",
    "t4_4_run_experiments()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "20e40d8fc09a6690434ad602c7eb2d8de15d36ec466bfbfb0de97c7c540d7363"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
